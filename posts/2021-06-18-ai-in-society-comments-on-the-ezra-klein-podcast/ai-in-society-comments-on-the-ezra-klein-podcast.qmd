---
title: "AI in society: comments on the Ezra Klein podcast"
description: |
  Tech utopianism, liberal fears, and challenging monopoly/power with democratic socialism
author:
  - name: Joshua Loftus
    url: https://joshualoftus.com/
date: 06-18-2021
output:
  distill::distill_article:
    self_contained: false
draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Power and control

I'll begin where I'm in agreement with Ezra and some of his guests. [Ted Chiang](https://www.nytimes.com/2021/03/30/opinion/ezra-klein-podcast-ted-chiang.html?showTranscript=1) put it well:

> **I tend to think that most fears about A.I. are best understood as fears about capitalism**. And I think that this is actually true of most fears of technology, too. Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it’s hard to distinguish the two.

This is an important insight that I would modify only slightly. Even though I am personally a democratic socialist, I think it's too specific to place the blame or fear entirely on capitalism. It's important to ask who is in control and who has power, and "capitalists" is one good and highly explanatory answer to that question- but it's not the only answer. I'm also afraid of the possibility *nobody is in control*, that the feedback loops between homo sapiens and its tools and culture that have already wreaked so much havoc on the planet are only going to continue their devastating expansion. And even in my ideal scenario--where all of humanity is actively engaged in democratic control, education, use, and creation of technology--**there will always be political struggles and contradictions, and technology/"AI" will be both subject of and tool in those struggles**.

Chiang was careful to say "most," so he is probably right. 

## Algorithms have no agency

This is such a basic point, but I think it's worth stressing and repeating because it is so often forgotten, especially in conversations about AI: **just because "intelligence" is in the name does not mean the thing is intelligent**.

It's an [open secret](https://twitter.com/ben_golub/status/1403776416051830788) that the discipline of Computer Science has a knack for giving things cool names because this makes [raising money](https://twitter.com/daniela_witten/status/1177330096530579456) easier. But this becomes a problem when people start uncritically believing the marketing copy, and the problem compounds when uncritical views are passed on to generations of impressionable students--some of whom may drop out of school to become tech CEOs without ever doing the homework of calculating some examples by hand and *viscerally* understanding the infinite dullness of the underlying mechanisms. It's glorified bookkeeping. 

["Agent-based" computing](https://en.wikipedia.org/wiki/Agent-based_model) and ["evolutionary" algorithms](https://en.wikipedia.org/wiki/Evolutionary_algorithm) and deep ["neural" networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are *names* for computational tools. Tools do not have agency, they are objects that actual agents use for their own purposes. If you don't believe an [abacus](https://en.wikipedia.org/wiki/Abacus) makes decisions then you should not believe that an *electric-powered abacus* makes decisions, and that's basically all that a computer is or ever will be. A complicated abacus that can convert a picture into numbers, do mathematical operations on those numbers, and convert the result into another picture... is not a living organism. This is true even if the output pictures are recognizable and meaningful to the living organisms using the machine. And it remains true even if thousands or millions of actual living organisms collectively work to "train" the machinery in the abacus to produce interesting input-output combinations. *It is still a lifeless machine*.

Works of art did not become sentient because painters learned good enough realism techniques to trick us into thinking we're looking at a photograph.

### So don't feel sorry for them

Klein has been repeating Chiang's concern to the effect that, I paraphrase: "long before we create AGI we will create *beings capable of suffering*, and given our track record on animal welfare this would likely become a horrific moral atrocity of mass suffering." No. Stop. We don't need to [imagine](https://thereader.mitpress.mit.edu/2020-the-year-of-robot-rights/) the suffering of some science fictional entities. There are enough actual living beings, including actual humans, who are already suffering and whose real plights demand our attention now. We can empathize with fictional characters to enjoy a good story, but at some point we have to close the book or turn off the screen and attend to real life.

### And don't forget who is in charge

Algorithms only ever do what they have been instructed to do. It's a false distinction--again just marketing copy--to say that AI or machine learning are a new kind of programming which is different from the old "rules based" paradigm. They take "training data" as an input? So do the algorithms that calculate a simple mean or a regression line. So does a sextant. If a computer language has abstractions that allow the programmer to write shorter and more human-readable code that does not mean that the *computer* has become more intelligent. (Matlab is not a paradigm shift relative to C because it has a simpler notation for doing matrix multiplication). If the output of a program is surprising to the engineers who designed it, that's just *bad engineering* (and potentially unsafe when used in the real world); it's a bug, not a feature.

I will probably have to make this point over and over forever because there's a massive industry profiting from the misconception. It's humans who have the agency. The simulacrum of intelligence we might perceive in the output of a computer program is just an echo of the actual intelligence of the humans who created the system, including its "training data." It's what David Donoho calls "[recycled intelligence](https://hdsr.mitpress.mit.edu/pub/rim3pvdw/release/6)."  

## Political economy of AGI

What's the point of "general purpose" AI? Why try to make it? Klein asked the CEO of OpenAI:

> EZRA KLEIN: Why try to make it generally intelligent at all? Geoff Hinton, one of the fathers of neural networks, he had this quote in the book "Genius Makers," that I recently read, where he just says, why do you want the robot digging your ditches to know about baseball? [...] Why not just create narrow worker machine learning programs?
>
> SAM ALTMAN: I have a lot of answers to this question, so I will start with a few. Number one, one thing that I really want is new knowledge creation. It is one thing to say you can have an A.I. that is as good as any human A.I. doctor. It is another to say you can have an A.I. that can solve all human disease in a way that humans are just not capable of doing. And I think you need a generally intelligent system to do that, to generate new knowledge, to learn new things, to do things humans can’t do. And it will need to pull together expertise across many different areas that no human is capable of holding in their brain at once — or even a team of humans — to do.
>
> So what a depressing thought to say that we’re going to limit ourselves to what humans are capable of rather than benefit from everything that we can build better tools for. We build tools so that we can do better than we can do with our hands digging up the dirt or whatever. All of this, everything we have here is because we started digging up the ground, finding stuff. And we made this room. We made this microphone. We made the internet. We have done incredible work with tools that have let us shoot past what we’d be capable of without them. And let’s not stop. That would really be depressing to me.
>
> Two is that someone’s going to do it. The upside of these systems are such that Geoff Hinton can certainly decide not to try to build generally intelligent systems, but someone’s going to do it. So I think there is no future that doesn’t have these systems in it. And so we have to talk about how we want to use them, what their rights are, what we want the world to look like, the universe to look like with them.

I don't believe this story. There are simpler explanations that are consistent with everything else we know about Silicon Valley in particular and post-industrial economics and politics in general. I'll give my own explanation in the next section, but first I want to critique Altman's views.

His first reason for building AGI is that it will do things humans are not capable of doing. He then goes on to point out how humanity has accomplished some great things by building and using tools. These are contradictory, because the first one is hiding the agency of the humans that would be *using* "AGI" as a tool to make great new discoveries or something. We don't say things like "microscopes solved disease," or 

### Fooling all the people, all the time

This is basically my definition for artificial general intelligence. We know it's not an AGI if its recycled intelligence fails to fool us some of the time. It's easier to create a customer service "chatbot" for a specific company because there are only a few topics that will cover most customers' questions. So it's easier to confuse or fool a customer into thinking they might be talking to a person rather than interacting with a glorified FAQ with built-in time delays for "typing." 

Consistent with my earlier point that algorithms and computers do not have agency or intelligence, 

### AGI = monopoly

