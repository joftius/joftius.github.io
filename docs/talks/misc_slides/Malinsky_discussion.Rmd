---
title: "Discussion of Malinsky @OCIS"
subtitle: ""
author: "Joshua Loftus (LSE Statistics)"
#institute: "LSE"
#date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "xaringan-themer.css", "../../../theme.css", "ninjutsu"]
#    seal: false    
    lib_dir: libs
    nature:
      titleSlideClass: ["bottom", "left"]
      countdown: 59000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
header-includes: 
  - \usepackage{tikz}
  - \usepackage{color}
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)
options(knitr.table.format = "html")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#2d708e",
  secondary_color = "#230433",
  link_color = "#55c667",
  text_bold_color = '#f68f46',
  title_slide_text_color = "#444444",
  title_slide_background_color = "#ffffff", #"#042333",
  title_slide_background_image = "", #../../files/theme/LSE/sign_reflection.jpg
  title_slide_background_size = "cover",
  ) #or contain
```

```{r xaringanextra, include=FALSE, warning=FALSE}
library(xaringanExtra)
#xaringanExtra::use_animate_all("slide_left")
xaringanExtra::use_tile_view()
xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs(font_family = "inherit")
```

```{r tidyverse, include=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
theme_set(theme_minimal(base_size = 22))
set.seed(1)
library(broom)
library(modelr)
```

<style type="text/css">
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
</style>

## Q: What is an explanation?

### A: Some philosophy

- I hope this paper *also* entices interest in the related philosophical literature

- What is a "dialectic?" I don't know but let's advance Breiman's "two cultures" one here. Efron and Hastie: algorithm vs inference

Algorithmic/prediction culture zooms ahead (on GPUs and crowdsourced labeled datasets, i.e. money)

Data/inference culture catching up slowly...

**Synthesis**(?): algorithms *for* inference

---

## Growth area for statistics?

- Now we have interpretable machine learning, explainable AI, etc.

- Problem: more complex expls./intrps. which are results of complex algorithms. **Can we trust them?** (*Who* can even understand them?...)

Re-draw the boundary of "algorithm" to include the explanation, require inference for this new super-algorithm

e.g. **Edge stability** in this paper. But could this come with any (probabilistic) guarantees?

---

### The most important limitation?

.center[
**Explaining $\hat Y \neq$ explaining $Y$**
]

- Paper reminds us: strength of assumptions, e.g. $Z$

- Maybe need to make this a catch phrase like "association is not (proof of) causation" 

- Diagnostic for $\hat Y$ algorithm? But then do we need to assume knowledge of PAG with $Z$ and $Y$ (for comparison)?

- Need both $Z$ and algorithm to capture real world?

(More dangerous for "causal" explanations? To me causality is about "nature," and the "natural cause" of why an algorithm does something is that somebody programmed it to do that)

---

### Simple (counter)example

Consider the simple case where $X = Z$, original features are interpretable already

Suppose the true causal DAG is $Y \to X = Z$

e.g. $Y$ an underlying health condition, $X$ symptoms

Most likely, $X$ causes $\hat Y$...

But $X$ does not cause $Y$

#### Could still be useful as a model diagnostic!

#### But what if our goal is to change the world?

(Not specific to this paper, any "causal explanation" of $\hat Y$)

---

## Questions

**Edge stability**: what does it mean? More generally, what *inferences* could we give *for* (these kinds of) *explanations*

**Explaining $Y$ vs $\hat Y$**: how does quality of explanation vary with quality of $Z$ and algorithm? e.g. if $\hat Y$ overfit or has poor OOD generalization

**Other assumptions/extensions**: are any (independence/Markov/faithfulness) assumptions (more likely to be) violated if we learn $Z$ from same data as $\hat Y$?


