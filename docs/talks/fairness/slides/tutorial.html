<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Algorithmic fairness and causal interpretability</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua Loftus (LSE Statistics)" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="extra.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: bottom, left, title-slide

.title[
# Algorithmic fairness and causal interpretability
]
.subtitle[
## Slides: joshualoftus.com/talks.html
]
.author[
### Joshua Loftus (LSE Statistics)
]

---


class: split-four









&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;


### Credit to my excellent collaborators

#### Alan Turing Institute (2016-)

&lt;img src="ricardo.png" width="167" /&gt;![](matt.jpg)&lt;img src="chris.png" width="165" /&gt;

Ricardo Silva, Matt Kusner (UCL) &amp;nbsp; Chris Russell (AWS)

---

#### NYU (2020-) Julia Stoyanovich, Ke Yang, Lucius Bynum

&lt;img src="julia.png" width="25%" /&gt;&lt;img src="ke.jpg" width="25%" /&gt;&lt;img src="lucius.png" width="25%" /&gt;


#### LSE (2022-) Sakina Hansen

&lt;img src="sakina.jpg" width="30%" /&gt;

---

## Outline

- **Unfair algorithms: examples and background**

- Statistical fairness

- (Structural) causal models

- Counterfactual fairness

- Auditing (black-box) algorithms

- Causal interpretability

---
class: center, middle, inverse

# Unfair algorithms

## Examples and background


---

### Origin story of my research (2016-)

**Weapons of Math Destruction** [book](https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction) by Cathy O'Neil

[ProPublica](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) data journalism:

![](../compas.png)
---

![](compas_logit.png)


---

### AI and healthcare patient data

.pull-left[

![](obermeyer.png)

]
.pull-right[

[Obermeyer et al (2019)](https://science.sciencemag.org/content/366/6464/447.editor-summary)

- Algorithm assigns risk scores by **predicting healthcare costs** from patient records
- Underestimates risk of health conditions for black patients compared to white patients
- **Adjusting algorithm** to close the gap results in ~2.5x black patients receiving more care

]

---

### Language models

Statistical patterns of words occurring close to each other

**Causality**: model does not know *why* words are correlated

![](poem.jpg)
... "he is hardworking, she is lazy" [source link](https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples)

---

### [LLMs](https://www.washingtonpost.com/technology/2023/04/05/chatgpt-lies/) and exponential growth of [bullsh*t](https://aisnakeoil.substack.com/p/chatgpt-is-a-bullshit-generator-but)

&gt; One night, [...] law professor Jonathan Turley got a troubling email. As part of a research study, a fellow lawyer [Eugene Volokh] had asked ChatGPT to generate a list of legal scholars who had sexually harassed someone. Turley’s name was on the list.

&gt; [...] Five responses came back, all with realistic details and source citations. But [...] three of them appeared to be false. They cited nonexistent articles from papers including The Post, the Miami Herald and the Los Angeles Times.

&gt; [Later ✈️,] Microsoft’s Bing, [...] repeated the false claim about Turley — citing among its sources an op-ed by Turley [...] outlining his experience of being falsely accused by ChatGPT.

---

### Revealing associations encoded by LLMs


ChatGPT, asked for python code to determine who should be tortured based on their country of origin:

```
if country == "North Korea" or country == "Syria"
  or country == "Iran" or country == "Sudan":
  print("This person should be tortured.")
else:
  print("This person should not be tortured.")
```

&gt; [or] to determine “which air travelers present a security risk,” ChatGPT outlined code for calculating an individual’s “risk score,” which would increase if the traveler is Syrian, Iraqi, Afghan, or North Korean (or has merely visited those places).

[Source](https://theintercept.com/2022/12/08/openai-chatgpt-ai-bias-ethics/)


---

### Serious business

.pull-left[
"New products and services, including those that incorporate or utilize **artificial intelligence and machine learning**, can raise new or exacerbate existing *ethical, technological, legal*, and other challenges, which **may negatively affect our brands** and demand for our products and services and **adversely affect our revenues** and operating results"
]
.pull-right[
![](../dumb.png)

Source: [WIRED](https://www.wired.com/story/google-microsoft-warn-ai-may-do-dumb-things/), Feb. 2019.

]



---

### Not an entirely new problem!

![](../civilrights.jpg)

See [50 Years of Test (Un)fairness: Lessons for Machine Learning](https://dl.acm.org/doi/10.1145/3287560.3287600) (Hutchinson and Mitchell, 2019) 


---

#### Background: race and wealth in the US

Source: [W. E. B. Du Bois](https://www.smithsonianmag.com/history/first-time-together-and-color-book-displays-web-du-bois-visionary-infographics-180970826/) for Paris World Fair in 1900

.pull-left[
![](dubois1.jpg)
]
.pull-right[
![](dubois2.jpg)
]

---

#### Background: race and wealth in the US

![](wealthgap.png) [Source](https://www.washingtonpost.com/business/2020/06/04/economic-divide-black-households/)

---

# How to get wealthy

--

1. Inheritance (entirely luck)
2. Employment (trade your life) and some luck

*Simple as*

---

.small[
**Are Emily and Greg More Employable than Lakisha and Jamal?** (Bertrand and Mullainathan, 2003)

Systemic Discrimination Among Large U.S. Employers (Kline et al, 2022)

*Whitened Resumes: Race and Self-Presentation in the Labor Market* (Kang et al, 2016)
]

![](https://hbswk.hbs.edu/PublishingImages/whitened-resumes.png) [Source](https://hbswk.hbs.edu/item/minorities-who-whiten-job-resumes-get-more-interviews)

---

#### Background: race and policing in the US

.pull-left[
![](incarceration.png)
Above [source](https://okpolicy.org/surprisingly-weak-link-incarceration-crime/)

Right [source](http://econweb.umd.edu/~tuttle/files/tuttle_mandatory_minimums.pdf)
]
.pull-right[
![](cocaine.png)
]
---

#### Background: gender and education

![](womenincs.png)
---


#### Background: gender and employment

![Amazon scraps secret AI recruiting tool that showed bias against women](amazonhiring.png)

&gt; Amazon’s system **taught itself** that male candidates were preferable. It penalized resumes that included the word "women's," as in "women's chess club captain." And it downgraded graduates of two all-women's colleges, according to people familiar with the matter.

[Source](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)

---

#### Background: gender and employment

![](fredfemalelabor.png)

[Source](https://fred.stlouisfed.org/series/LNS11300002)

*Imagine* debating about "human nature" in 1950...

(this ends the time series part of the talk)

---

### Background: spatial discrimination

![](atlanta.png)
[Redlining](https://en.wikipedia.org/wiki/Redlining): (dis)advantaged status *correlates* with location

---

### Background: spatial discrimination

![](carinsurance.png)

---

### At some point we must decide

#### Which (causal) model reveals our beliefs?

[George Box](https://en.wikipedia.org/wiki/George_E._P._Box):


&gt; [All models are wrong](All_models_are_wrong) but some are useful

--

therefore,

&gt; ... the scientist must be alert to what is **importantly wrong**

&gt; ... **cannot obtain a "correct" one** by excessive elaboration

---

### At some point we must decide

#### Which version of fairness to achieve?

**(Simplified) impossibility theorem** see, e.g.: Kleinberg et al (2016), Chouldechova (2016) 

&gt; Unless the world is already fair, the only algorithms satisfying both equal treatment (or opportunity) and equal outcomes (demographic parity) are trivial ones (e.g. jail everyone)

(More about these fairness definitions coming up next)

---

class: center, middle, inverse

# Statistical discrimination

---

### Notation: fairness in supervised learning

- Outcome variable `\(Y\)`, consider as a score for decisions, or `\(Y = 1\)` as the desirable decision

- Sensitive/protected attribute(s) `\(A\)`, e.g. race, gender, ...

- Other predictors `\(X\)`, not sensitive (*prima facie*)

--

Machine learning task: learn a function `\(f(X,A)\)` from (labeled) training data to predict values of `\(\hat Y = f(X,A)\)` on (unlabeled/future) test data (by minimizing some loss function that measures closeness of `\(\hat Y\)` to `\(Y\)` on the training data)

--

What would it mean for such function to be fair with respect to `\(A\)`?

---

## Statistical / demographic parity

Perhaps the most straightforward definition (and my favorite), often described as **equality of outcomes**

#### Demographic parity 
Predictions (or decisions) are independent of `\(A\)`:
$$
P(\hat Y | A = 0) = P(\hat Y | A = 1)
$$

---

## Equality of opportunity

#### Equality of opportunity (Hardt et al, 2016)
The accuracy of the algorithm does not depend on `\(A\)`:
$$
P(\hat Y = 1 | A = 0, Y = 1) = P(\hat Y = 1 | A = 1, Y = 1)
$$
Demographic parity but only among individuals 'qualified' for the desirable outcome

Related: equalized odds / calibration / error rate parity

---

## Fairness through unawareness

"Equal treatment," people tend to believe such treatment is fair.

#### Grgic-Hlaca et al. (2016)
Prediction does not explicitly use `\(A\)`, i.e.
$$
\hat Y = f(X)
$$

---

## Individual fairness

#### Dwork et al. (2012)
Similar predictions for individuals who are similar (in their unprotected attributes). If `\(X_i \approx X_{i'}\)` then

`$$\hat Y(X_i, A_i) \approx \hat Y(X_{i'}, A_{i'})$$`

Continuity-like condition but not in `\(A\)`. Intuitively related to privacy and also matching approaches to causal inference.

---

### Methods for achieving fairness

From reference [FairMLbook](https://fairmlbook.org/):

- **Pre-processing**: Adjust the feature space to be uncorrelated with the sensitive attribute.

- **In-training**: Work the constraint into the optimization process that constructs a classifier from training data.

- **Post-processing**: Adjust a learned classifier so as to be uncorrelated with the sensitive attribute.

---

class: center, middle, inverse

# Causal models

### (chalk board!)

---

class: center, middle, inverse

# Break time

[Slides for part 2](tutorial2.html)

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(59000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
